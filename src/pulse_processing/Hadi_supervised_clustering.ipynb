{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from util.bat import *\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set some Pandas options\n",
    "pd.set_option('notebook_repr_html', True)\n",
    "pd.set_option('max_columns', 30)\n",
    "pd.set_option('max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yang_cluster():\n",
    "    info=extract_anabat('../data/P7132033.37#')\n",
    "    t=info[0]\n",
    "    freq=info[1]\n",
    "    ampl=info[2]\n",
    "    metadata=info[3]\n",
    "    pulses=remove_noise2(t,freq)\n",
    "    \n",
    "    b,c=get_dy_dy2(pulses)\n",
    "    \n",
    "    bf=get_features(b)\n",
    "    cf=get_features(c)\n",
    "    \n",
    "    bf.iloc[8:11,]\n",
    "    cf.iloc[8:11,]\n",
    "    ff=pd.concat([bf, cf.iloc[:,0:2]], axis=1)\n",
    "    ff.iloc[8:11,]\n",
    "    \n",
    "    # Clustering a sample recording pulses using KMeans\n",
    "    est = KMeans(3)  # 4 clusters\n",
    "    # X=b.iloc[:,0:2]\n",
    "    X=ff\n",
    "    est.fit(X)\n",
    "    y_kmeans = est.predict(X)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c = y_kmeans, s=50, cmap='rainbow');\n",
    "\n",
    "    display_pulses(pulses, len(pulses), 5, rand_flag = False, cluster = y_kmeans)\n",
    "    return pulses, y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulses, labels = yang_cluster()\n",
    "\n",
    "pulses = pulses[:len(labels)]\n",
    "\n",
    "for i, pulse in enumerate(pulses):\n",
    "    if labels[i] == 2:\n",
    "        labels[i] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Following a tutorial from torres.ai\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from IPython.display import SVG\n",
    "\n",
    "# Load data from MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Convert to float32 values between 0 and 1\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Reshape to vectors in R1 of length 784\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Reshape to vectors in R1 of length 10\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=10, epochs=40)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "predictions = model.predict(x_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN tutorial\n",
    "# https://towardsdatascience.com/convolutional-neural-networks-for-beginners-practical-guide-with-python-and-keras-dc688ea90dca\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          verbose=1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "filenames = glob.glob('..\\\\..\\\\data\\\\sorted\\\\**\\\\*.png', recursive=True)\n",
    "data = list()\n",
    "\n",
    "label = {\n",
    "        'echolocation': 0,\n",
    "        'abnormal': 1,\n",
    "        'noise': 2\n",
    "    }\n",
    "\n",
    "for filename in filenames:\n",
    "    lbl = filename.rsplit('\\\\')[-2]    \n",
    "    img = np.array(Image.open(filename))\n",
    "    data.append([label.get(lbl), img])\n",
    "    \n",
    "shuffle(data)\n",
    "\n",
    "train = data[:int(len(data)*.8)]\n",
    "test = data[int(len(data)*.2):]\n",
    "\n",
    "train_labels = [dat[0] for dat in train]\n",
    "train_images = [dat[1] for dat in train]\n",
    "\n",
    "test_labels = [dat[0] for dat in test]\n",
    "test_images = [dat[1] for dat in test]\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, batch_size=100, epochs=5, verbose=1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://adventuresinmachinelearning.com/python-tensorflow-tutorial/\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Python optimisation variables\n",
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# declare the training data placeholders\n",
    "# input x - for 28 x 28 pixels = 784\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# now declare the output data placeholder - 10 digits\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# now declare the weights connecting the input to the hidden layer\n",
    "W1 = tf.Variable(tf.random_normal([784, 200], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([200]), name='b1')\n",
    "\n",
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random_normal([200, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "\n",
    "# calculate the output of the hidden layer\n",
    "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)\n",
    "\n",
    "# now calculate the hidden layer output - in this case, let's use a softmax activated output layer\n",
    "y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))\n",
    "y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999) # prevent NaN from log(0)\n",
    "\n",
    "#calculate how messed up the NN is\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped) + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "# add an optimiser\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# finally setup the initialisation operator\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "   # initialise the variables\n",
    "   sess.run(init_op)\n",
    "   total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "   for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "   print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext_9:0\", shape=(?, ?, 1), dtype=float32)\n",
      "Tensor(\"IteratorGetNext_9:1\", shape=(2,), dtype=float32)\n",
      "Tensor(\"Relu_9:0\", shape=(2,), dtype=float32)\n",
      "Epoch: 0, loss: 1.414, training accuracy: 52.73%\n",
      "Epoch: 1, loss: 1.303, training accuracy: 41.33%\n",
      "Epoch: 2, loss: 0.703, training accuracy: 182.19%\n",
      "Epoch: 3, loss: 0.701, training accuracy: 193.45%\n",
      "Epoch: 4, loss: 5.765, training accuracy: 303.26%\n",
      "Epoch: 5, loss: 1.310, training accuracy: 41.99%\n",
      "Epoch: 6, loss: 1.335, training accuracy: 44.71%\n",
      "Epoch: 7, loss: 0.703, training accuracy: 182.08%\n",
      "Epoch: 8, loss: 0.701, training accuracy: 193.69%\n",
      "Epoch: 9, loss: 0.699, training accuracy: 203.48%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "datadir = '../../data/sorted/'\n",
    "    \n",
    "train = list()\n",
    "for file in glob.glob(datadir + 'train_abnormal/*.png'):\n",
    "    train.append([file, 0])\n",
    "\n",
    "for file in glob.glob(datadir + 'train_echolocation/*.png'):\n",
    "    train.append([file, 1])\n",
    "\n",
    "test = list()\n",
    "for file in glob.glob(datadir + 'test_abnormal/*.png'):\n",
    "    test.append([file, 0])\n",
    "\n",
    "for file in glob.glob(datadir + 'test_echolocation/*.png'):\n",
    "    test.append([file, 1])\n",
    "\n",
    "\n",
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# Image to 1d array, label to 2vector\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image = tf.image.decode_png(image_string, channels=1)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    #tf.reshape(image, [-1])\n",
    "    label = tf.one_hot(label, 2)\n",
    "    return image, label\n",
    "\n",
    "training = tf.data.Dataset.from_tensor_slices(([row[0] for row in train], [row[1] for row in train]))\n",
    "testing = tf.data.Dataset.from_tensor_slices(([row[0] for row in test], [row[1] for row in test]))\n",
    "\n",
    "training = training.map(_parse_function).shuffle(5000)\n",
    "testing = testing.map(_parse_function).shuffle(5000)\n",
    "\n",
    "iter = tf.data.Iterator.from_structure(training.output_types, training.output_shapes)\n",
    "next_element = iter.get_next()\n",
    "train_init_op = iter.make_initializer(training)\n",
    "test_init_op = iter.make_initializer(testing)\n",
    "\n",
    "def nn_model(in_data):\n",
    "    bn = tf.layers.batch_normalization(in_data)\n",
    "    fc1 = tf.layers.dense(bn, 1000)\n",
    "    fc2 = tf.layers.dense(fc1, 300)\n",
    "    fc3 = tf.layers.dense(fc2, 2)\n",
    "    fc4 = tf.reduce_mean(fc3, axis = 0)\n",
    "    fc5 = tf.reduce_mean(fc4, axis = 0)\n",
    "    fc6 = tf.layers.dropout(fc5)\n",
    "    fc7 = tf.nn.relu(fc6)\n",
    "    return fc7\n",
    "\n",
    "print(next_element[0])\n",
    "print(next_element[1])\n",
    "# create the neural network model\n",
    "logits = nn_model(next_element[0])\n",
    "print(logits)\n",
    "\n",
    "# add the optimizer and loss\n",
    "loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=next_element[1], logits=logits))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# get accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(next_element[1], 0), tf.argmax(logits, 0))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.math.abs(tf.math.subtract(tf.cast(logits, tf.float32), next_element[1])), tf.float32))\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(train_init_op)\n",
    "    \n",
    "    for x in range(epochs):\n",
    "        l, _, acc = sess.run([loss, optimizer, accuracy])\n",
    "        print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(x, l, acc * 100))\n",
    "    \n",
    "    sess.run(test_init_op)\n",
    "    avg_acc = 0\n",
    "    for i in range(100):\n",
    "        acc = sess.run([accuracy])\n",
    "        avg_acc += acc[0]\n",
    "    \n",
    "    print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(100, (avg_acc / 100) * 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
