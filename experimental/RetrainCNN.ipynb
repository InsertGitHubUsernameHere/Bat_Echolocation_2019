{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, shutil\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "#Variables to edit---------------\n",
    "epochs = 1 #Number of epochs\n",
    "learningrate = 0.0005 #Learning rate for optimizer\n",
    "imageheight=200 #Image height for model\n",
    "imagewidth=300  #Image width for model\n",
    "cutoff=0.7 #Above which is an echolcation and below is abnormal file\n",
    "mfolder = \"./weights1/\" #Folder with saved models in h5 format\n",
    "afolder = \"./sorted/predicta/ab/\" #Image folder with images to test on model\n",
    "efolder = \"./sorted/ce/e/\" #Image folder with images to test on model\n",
    "#End of variables to edit---------\n",
    "if not os.path.exists('/models1'): #Create folder for models\n",
    "    os.makedirs('/models1')\n",
    "#Tensorboard functions\n",
    "checkpointvacc = ModelCheckpoint('models1/{epoch:02d}.h5', monitor='val_acc', mode='max',\n",
    "                                 save_best_only=True, verbose=0)\n",
    "checkpointvloss = ModelCheckpoint('models1/{epoch:02d}.h5', monitor='val_loss', mode='min',\n",
    "                                 save_best_only=True, verbose=0)\n",
    "#Setup to stream images for test and training\n",
    "train_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_data.flow_from_directory('sorted/train',batch_size=45,class_mode='binary', target_size=(200,300))\n",
    "test_generator = test_data.flow_from_directory('sorted/test',batch_size=16,class_mode='binary',target_size=(200,300))\n",
    "#Create model layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(96, (5, 5), strides=5, input_shape=(200, 300, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(128, (5, 5), strides=5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))      \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#Set model parameters\n",
    "adam = optimizers.adam(lr=learningrate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "history=model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=40,\n",
    "        epochs=epochs,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=50,\n",
    "        callbacks=[checkpointvloss,checkpointvacc])\n",
    "\n",
    "#Evaluate model---------------------\n",
    "for mdl in range(16): #Remove unwanted models\n",
    "    if os.path.exists(mfolder+str(mdl)+\".h5\"):\n",
    "        os.remove(mfolder+str(mdl)+\".h5\")\n",
    "    if os.path.exists(mfolder+\"0\"+str(mdl)+\".h5\"):\n",
    "        os.remove(mfolder+\"0\"+str(mdl)+\".h5\")\n",
    "shutil.copy2('/models/CNN200x300b.h5', '/models1')       \n",
    "mfilelist = [f for f in os.listdir(mfolder) if os.path.isfile(os.path.join(mfolder, f))] #Make list of model filenames\n",
    "afilelist = [f for f in os.listdir(afolder) if os.path.isfile(os.path.join(afolder, f))] #Make list of image filenames\n",
    "efilelist = [f for f in os.listdir(efolder) if os.path.isfile(os.path.join(efolder, f))] #Make list of image filenames\n",
    "abcount=echcount=abest=ambest=0\n",
    "abmodel = [0] * len(mfilelist)\n",
    "for mdl in range(len(mfilelist)):\n",
    "    model = load_model(mfolder+mfilelist[mdl])\n",
    "    for inum in range(len(afilelist)):#Go through images 1 by 1 and run predictions on model\n",
    "        image = load_img(afolder + afilelist[inum], target_size=(imageheight,imagewidth))\n",
    "        imageresize = image.resize((imagewidth, imageheight)) #Resize image for desired CNN model\n",
    "        imagearray = img_to_array(imageresize) #Make image into an array\n",
    "        imagearray = np.expand_dims(imagearray, 0) #Add 4th dimension to array for CNN -> will get error otherwise\n",
    "        imagearray = imagearray.astype('float32')/255 #Convert array so that its predictions are readable\n",
    "        prediction=model.predict(imagearray) #Run prediction on image-Returns list of float predictions\n",
    "        if prediction[0]<cutoff:\n",
    "            abcount=abcount+1\n",
    "    abmodel[mdl]= (abcount*100)/len(afilelist)\n",
    "    if ((abcount*100)/len(afilelist))>=abest:\n",
    "        abest = (abcount*100)/len(afilelist)\n",
    "        ambest = mdl\n",
    "   \n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
