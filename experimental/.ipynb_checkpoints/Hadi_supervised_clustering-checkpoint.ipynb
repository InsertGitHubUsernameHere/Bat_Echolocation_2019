{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from util.bat import *\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set some Pandas options\n",
    "pd.set_option('notebook_repr_html', True)\n",
    "pd.set_option('max_columns', 30)\n",
    "pd.set_option('max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yang_cluster():\n",
    "    info=extract_anabat('../data/P7132033.37#')\n",
    "    t=info[0]\n",
    "    freq=info[1]\n",
    "    ampl=info[2]\n",
    "    metadata=info[3]\n",
    "    pulses=remove_noise2(t,freq)\n",
    "    \n",
    "    b,c=get_dy_dy2(pulses)\n",
    "    \n",
    "    bf=get_features(b)\n",
    "    cf=get_features(c)\n",
    "    \n",
    "    bf.iloc[8:11,]\n",
    "    cf.iloc[8:11,]\n",
    "    ff=pd.concat([bf, cf.iloc[:,0:2]], axis=1)\n",
    "    ff.iloc[8:11,]\n",
    "    \n",
    "    # Clustering a sample recording pulses using KMeans\n",
    "    est = KMeans(3)  # 4 clusters\n",
    "    # X=b.iloc[:,0:2]\n",
    "    X=ff\n",
    "    est.fit(X)\n",
    "    y_kmeans = est.predict(X)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c = y_kmeans, s=50, cmap='rainbow');\n",
    "\n",
    "    display_pulses(pulses, len(pulses), 5, rand_flag = False, cluster = y_kmeans)\n",
    "    return pulses, y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulses, labels = yang_cluster()\n",
    "\n",
    "pulses = pulses[:len(labels)]\n",
    "\n",
    "for i, pulse in enumerate(pulses):\n",
    "    if labels[i] == 2:\n",
    "        labels[i] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Following a tutorial from torres.ai\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from IPython.display import SVG\n",
    "\n",
    "# Load data from MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Convert to float32 values between 0 and 1\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Reshape to vectors in R1 of length 784\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Reshape to vectors in R1 of length 10\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=10, epochs=40)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "predictions = model.predict(x_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN tutorial\n",
    "# https://towardsdatascience.com/convolutional-neural-networks-for-beginners-practical-guide-with-python-and-keras-dc688ea90dca\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          verbose=1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "filenames = glob.glob('..\\\\..\\\\data\\\\sorted\\\\**\\\\*.png', recursive=True)\n",
    "data = list()\n",
    "\n",
    "label = {\n",
    "        'echolocation': 0,\n",
    "        'abnormal': 1,\n",
    "        'noise': 2\n",
    "    }\n",
    "\n",
    "for filename in filenames:\n",
    "    lbl = filename.rsplit('\\\\')[-2]    \n",
    "    img = np.array(Image.open(filename))\n",
    "    data.append([label.get(lbl), img])\n",
    "    \n",
    "shuffle(data)\n",
    "\n",
    "train = data[:int(len(data)*.8)]\n",
    "test = data[int(len(data)*.2):]\n",
    "\n",
    "train_labels = [dat[0] for dat in train]\n",
    "train_images = [dat[1] for dat in train]\n",
    "\n",
    "test_labels = [dat[0] for dat in test]\n",
    "test_images = [dat[1] for dat in test]\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, batch_size=100, epochs=5, verbose=1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-5b78f4fa833c>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: 1 cost = 0.557\n",
      "Epoch: 2 cost = 0.213\n",
      "Epoch: 3 cost = 0.153\n",
      "Epoch: 4 cost = 0.123\n",
      "Epoch: 5 cost = 0.098\n",
      "Epoch: 6 cost = 0.077\n",
      "Epoch: 7 cost = 0.063\n",
      "Epoch: 8 cost = 0.050\n",
      "Epoch: 9 cost = 0.046\n",
      "Epoch: 10 cost = 0.035\n",
      "0.9787\n"
     ]
    }
   ],
   "source": [
    "#https://adventuresinmachinelearning.com/python-tensorflow-tutorial/\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Python optimisation variables\n",
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# declare the training data placeholders\n",
    "# input x - for 28 x 28 pixels = 784\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# now declare the output data placeholder - 10 digits\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# now declare the weights connecting the input to the hidden layer\n",
    "W1 = tf.Variable(tf.random_normal([784, 200], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([200]), name='b1')\n",
    "\n",
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random_normal([200, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "\n",
    "# calculate the output of the hidden layer\n",
    "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)\n",
    "\n",
    "# now calculate the hidden layer output - in this case, let's use a softmax activated output layer\n",
    "y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))\n",
    "y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999) # prevent NaN from log(0)\n",
    "\n",
    "#calculate how messed up the NN is\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped) + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "# add an optimiser\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# finally setup the initialisation operator\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "   # initialise the variables\n",
    "   sess.run(init_op)\n",
    "   total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "   for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "   print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext_13:0\", shape=(256, 256, 1), dtype=float32)\n",
      "Tensor(\"IteratorGetNext_13:1\", shape=(2,), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv2d_4 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [256, 256, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a977ffd13871>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;31m# create the neural network model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-a977ffd13871>\u001b[0m in \u001b[0;36mnn_model\u001b[1;34m(in_data)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'channels_last'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m96\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mm1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[0;32m    415\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m       _scope=name)\n\u001b[1;32m--> 417\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    815\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \"\"\"\n\u001b[1;32m--> 817\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m         \u001b[1;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1475\u001b[0m                            \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full shape received: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1477\u001b[1;33m                            str(x.shape.as_list()))\n\u001b[0m\u001b[0;32m   1478\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m         \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer conv2d_4 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [256, 256, 1]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "datadir = '../../data/sorted/'\n",
    "    \n",
    "train = list()\n",
    "for file in glob.glob(datadir + 'train_abnormal/*.png'):\n",
    "    train.append([file, 0])\n",
    "\n",
    "for file in glob.glob(datadir + 'train_echolocation/*.png'):\n",
    "    train.append([file, 1])\n",
    "\n",
    "test = list()\n",
    "for file in glob.glob(datadir + 'test_abnormal/*.png'):\n",
    "    test.append([file, 0])\n",
    "\n",
    "for file in glob.glob(datadir + 'test_echolocation/*.png'):\n",
    "    test.append([file, 1])\n",
    "\n",
    "\n",
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# Image to 1d array, label to 2vector\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image = tf.image.decode_png(image_string, channels=1)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize_images(image, (256, 256, 1))\n",
    "    label = tf.one_hot(label, 2)\n",
    "    return image, label\n",
    "\n",
    "training = tf.data.Dataset.from_tensor_slices(([row[0] for row in train], [row[1] for row in train]))\n",
    "testing = tf.data.Dataset.from_tensor_slices(([row[0] for row in test], [row[1] for row in test]))\n",
    "\n",
    "training = training.map(_parse_function)\n",
    "testing = testing.map(_parse_function)\n",
    "\n",
    "iter = tf.data.Iterator.from_structure(training.output_types, training.output_shapes)\n",
    "next_element = iter.get_next()\n",
    "train_init_op = iter.make_initializer(training)\n",
    "test_init_op = iter.make_initializer(testing)\n",
    "\n",
    "def nn_model(in_data):\n",
    "    c1 = tf.layers.conv2d(in_data, data_format = 'channels_last', filters = 96, kernel_size = (7, 7), strides = 3)\n",
    "    a1 = tf.nn.relu(c1)\n",
    "    m1 = tf.layers.MaxPooling2D(a1, pool_size = (4, 4), strides = 2)\n",
    "    z1 = tf.keras.layers.ZeroPadding2D(m1, 1)\n",
    "    \n",
    "    c2 = tf.layers.conv2d(z1, 128, (3, 3), strides = 3)\n",
    "    a2 = tf.nn.relu(c2)\n",
    "    m2 = tf.layers.MaxPooling2D(a2, pool_size = (3, 3), strides = 2)\n",
    "    z2 = tf.keras.layers.ZeroPadding2D(m2, 1)\n",
    "    \n",
    "    c3 = tf.layers.conv2d(z2, 256, (3, 3), strides = 3)\n",
    "    \n",
    "    f1 = tf.layers.Flatten()(c3)\n",
    "    d1 = tf.layers.dense(f1, 256, activation = 'relu')\n",
    "    dr1 = tf.layers.dropout(d1, rate = 0.5)\n",
    "    d2 = tf.layers.dense(dr1, 1, activation = 'sigmoid')\n",
    "    #bn = tf.layers.batch_normalization(in_data)\n",
    "    #fc1 = tf.layers.dense(bn, 1000)\n",
    "    #fc2 = tf.layers.dense(fc1, 300)\n",
    "    #fc3 = tf.layers.dense(fc2, 2)\n",
    "    #fc4 = tf.reduce_mean(fc3, axis = 0)\n",
    "    #fc5 = tf.reduce_mean(fc4, axis = 0)\n",
    "    #fc6 = tf.layers.dropout(fc5)\n",
    "    #fc7 = tf.nn.relu(fc6)\n",
    "    return fc7\n",
    "\n",
    "print(next_element[0])\n",
    "print(next_element[1])\n",
    "# create the neural network model\n",
    "logits = nn_model(next_element[0])\n",
    "print(logits)\n",
    "\n",
    "# add the optimizer and loss\n",
    "loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=next_element[1], logits=logits))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# get accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(next_element[1], 0), tf.argmax(logits, 0))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.math.abs(tf.math.subtract(tf.cast(logits, tf.float32), next_element[1])), tf.float32))\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(train_init_op)\n",
    "    \n",
    "    for x in range(epochs):\n",
    "        l, _, acc = sess.run([loss, optimizer, accuracy])\n",
    "        print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(x, l, acc * 100))\n",
    "    \n",
    "    sess.run(test_init_op)\n",
    "    avg_acc = 0\n",
    "    for i in range(100):\n",
    "        acc = sess.run([accuracy])\n",
    "        avg_acc += acc[0]\n",
    "    \n",
    "    print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(100, (avg_acc / 100) * 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
